# ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Annotation (Data Format Guide)

## üìã ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå Annotation

### ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á PaddleOCR
‡πÑ‡∏ü‡∏•‡πå annotation ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå text (.txt) ‡πÇ‡∏î‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
```
image_path\t[{"transcription": "text", "points": [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]}]
```

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Annotation
```
images/train_001.jpg	[{"transcription": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "points": [[100, 50], [200, 50], [200, 80], [100, 80]]}, {"transcription": "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö", "points": [[100, 90], [250, 90], [250, 120], [100, 120]]}]
images/train_002.jpg	[{"transcription": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö", "points": [[50, 30], [300, 30], [300, 60], [50, 60]]}]
images/train_003.jpg	[{"transcription": "1234567890", "points": [[80, 100], [180, 100], [180, 130], [80, 130]]}]
```

## üñºÔ∏è ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö

### ‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
- **‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö**: JPG, JPEG, PNG, TIFF, BMP
- **‡∏Ç‡∏ô‡∏≤‡∏î**: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 640x480 ‡∏ñ‡∏∂‡∏á 2048x2048 pixels
- **Aspect Ratio**: ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î ‡πÅ‡∏ï‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 10:1
- **Color Space**: RGB ‡∏´‡∏£‡∏∑‡∏≠ Grayscale

### ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
```python
import cv2
import numpy as np

def preprocess_image(image_path, target_size=(1024, 1024)):
    """
    ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    """
    image = cv2.imread(image_path)
    
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    h, w = image.shape[:2]
    if max(h, w) > target_size[0]:
        scale = target_size[0] / max(h, w)
        new_h, new_w = int(h * scale), int(w * scale)
        image = cv2.resize(image, (new_w, new_h))
    
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    image = cv2.bilateralFilter(image, 9, 75, 75)
    
    return image
```

## üìä ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Text Detection
```json
[
  {
    "transcription": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ detect",
    "points": [
      [x1, y1],  // ‡∏°‡∏∏‡∏°‡∏ã‡πâ‡∏≤‡∏¢‡∏ö‡∏ô
      [x2, y2],  // ‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô
      [x3, y3],  // ‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏•‡πà‡∏≤‡∏á
      [x4, y4]   // ‡∏°‡∏∏‡∏°‡∏ã‡πâ‡∏≤‡∏¢‡∏•‡πà‡∏≤‡∏á
    ]
  }
]
```

### ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î (Points)
- **‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î**: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ 4 ‡∏à‡∏∏‡∏î‡πÄ‡∏™‡∏°‡∏≠ (quadrilateral)
- **‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏∏‡∏î**: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏Ç‡πá‡∏°‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏°‡∏∏‡∏°‡∏ã‡πâ‡∏≤‡∏¢‡∏ö‡∏ô
- **‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏¥‡∏Å‡∏±‡∏î**: (0,0) ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏∏‡∏°‡∏ã‡πâ‡∏≤‡∏¢‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
- **‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**: integer ‡∏´‡∏£‡∏∑‡∏≠ float

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Points
```python
# ‡∏£‡∏π‡∏õ‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°‡∏°‡∏∏‡∏°‡∏â‡∏≤‡∏Å
points_rectangle = [[100, 50], [300, 50], [300, 100], [100, 100]]

# ‡∏£‡∏π‡∏õ‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°‡πÄ‡∏≠‡∏µ‡∏¢‡∏á
points_rotated = [[120, 40], [280, 60], [270, 110], [110, 90]]

# ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏á
points_curved = [[100, 50], [300, 45], [305, 95], [95, 100]]
```

## üîß ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Annotation

### 1. ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ LabelMe
```python
def convert_labelme_to_paddleocr(labelme_json_path, image_base_dir):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå LabelMe JSON ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö PaddleOCR
    """
    import json
    
    with open(labelme_json_path, 'r', encoding='utf-8') as f:
        labelme_data = json.load(f)
    
    image_path = labelme_data['imagePath']
    annotations = []
    
    for shape in labelme_data['shapes']:
        if shape['shape_type'] == 'polygon' and len(shape['points']) >= 4:
            annotation = {
                'transcription': shape['label'],
                'points': shape['points'][:4]  # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 4 ‡∏à‡∏∏‡∏î‡πÅ‡∏£‡∏Å
            }
            annotations.append(annotation)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î annotation
    relative_path = os.path.relpath(image_path, image_base_dir)
    annotation_line = f"{relative_path}\t{json.dumps(annotations, ensure_ascii=False)}"
    
    return annotation_line
```

### 2. ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ CVAT (Computer Vision Annotation Tool)
```python
def convert_cvat_to_paddleocr(cvat_xml_path):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå CVAT XML ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö PaddleOCR
    """
    import xml.etree.ElementTree as ET
    
    tree = ET.parse(cvat_xml_path)
    root = tree.getroot()
    
    annotations = {}
    
    for image in root.findall('.//image'):
        image_name = image.get('name')
        image_annotations = []
        
        for polygon in image.findall('.//polygon'):
            points_str = polygon.get('points')
            label = polygon.get('label', '')
            
            # ‡πÅ‡∏õ‡∏•‡∏á points string ‡πÄ‡∏õ‡πá‡∏ô list
            points = []
            for point_str in points_str.split(';'):
                x, y = map(float, point_str.split(','))
                points.append([int(x), int(y)])
            
            if len(points) >= 4:
                annotation = {
                    'transcription': label,
                    'points': points[:4]
                }
                image_annotations.append(annotation)
        
        if image_annotations:
            annotations[image_name] = image_annotations
    
    return annotations
```

## ‚úÖ ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

### 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Annotation
```python
def validate_annotation_quality(annotation_file):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå annotation
    """
    issues = []
    
    with open(annotation_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            try:
                parts = line.split('\t')
                if len(parts) != 2:
                    issues.append(f"Line {line_num}: Invalid format")
                    continue
                
                image_path, annotation_json = parts
                annotations = json.loads(annotation_json)
                
                for i, ann in enumerate(annotations):
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö required fields
                    if 'transcription' not in ann:
                        issues.append(f"Line {line_num}, annotation {i}: Missing transcription")
                    
                    if 'points' not in ann:
                        issues.append(f"Line {line_num}, annotation {i}: Missing points")
                        continue
                    
                    points = ann['points']
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î
                    if len(points) != 4:
                        issues.append(f"Line {line_num}, annotation {i}: Must have 4 points")
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏∏‡∏î
                    for j, point in enumerate(points):
                        if not isinstance(point, list) or len(point) != 2:
                            issues.append(f"Line {line_num}, annotation {i}, point {j}: Invalid point format")
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏à‡∏∏‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
                    if len(points) == 4:
                        area = calculate_polygon_area(points)
                        if area < 10:  # ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                            issues.append(f"Line {line_num}, annotation {i}: Text area too small")
            
            except json.JSONDecodeError:
                issues.append(f"Line {line_num}: Invalid JSON")
            except Exception as e:
                issues.append(f"Line {line_num}: {str(e)}")
    
    return issues

def calculate_polygon_area(points):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°
    """
    x = [p[0] for p in points]
    y = [p[1] for p in points]
    return 0.5 * abs(sum(x[i]*y[i+1] - x[i+1]*y[i] for i in range(-1, len(x)-1)))
```

### 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
```python
def analyze_dataset_distribution(annotation_file):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á dataset
    """
    text_lengths = []
    text_counts_per_image = []
    character_frequency = {}
    
    with open(annotation_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            try:
                parts = line.split('\t')
                annotations = json.loads(parts[1])
                
                image_text_count = len(annotations)
                text_counts_per_image.append(image_text_count)
                
                for ann in annotations:
                    text = ann['transcription']
                    text_lengths.append(len(text))
                    
                    # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                    for char in text:
                        character_frequency[char] = character_frequency.get(char, 0) + 1
                        
            except:
                continue
    
    # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    stats = {
        'total_images': len(text_counts_per_image),
        'total_text_instances': sum(text_counts_per_image),
        'avg_text_per_image': np.mean(text_counts_per_image),
        'avg_text_length': np.mean(text_lengths),
        'unique_characters': len(character_frequency),
        'most_common_chars': sorted(character_frequency.items(), key=lambda x: x[1], reverse=True)[:20]
    }
    
    return stats
```

## üìù ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Recognition

### ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Annotation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Text Recognition
```
cropped_image_path\ttext_content
```

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Recognition Annotation
```
rec_images/word_001.jpg	‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ
rec_images/word_002.jpg	‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö
rec_images/word_003.jpg	1234567890
rec_images/word_004.jpg	ABC-123
```

### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Recognition ‡∏à‡∏≤‡∏Å Detection Data
```python
def create_recognition_dataset(detection_annotation_file, image_dir, output_dir):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö recognition ‡∏à‡∏≤‡∏Å detection annotations
    """
    import cv2
    import os
    
    os.makedirs(output_dir, exist_ok=True)
    rec_annotations = []
    
    with open(detection_annotation_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            
            try:
                image_path, annotation_json = line.split('\t')
                annotations = json.loads(annotation_json)
                
                # ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                full_image_path = os.path.join(image_dir, image_path)
                image = cv2.imread(full_image_path)
                
                if image is None:
                    continue
                
                for ann_idx, ann in enumerate(annotations):
                    text = ann['transcription']
                    points = ann['points']
                    
                    # ‡∏ï‡∏±‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡∏≤‡∏° bounding box
                    cropped = crop_text_region(image, points)
                    
                    if cropped is not None:
                        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
                        crop_filename = f"crop_{line_num:06d}_{ann_idx:03d}.jpg"
                        crop_path = os.path.join(output_dir, crop_filename)
                        cv2.imwrite(crop_path, cropped)
                        
                        # ‡πÄ‡∏û‡∏¥‡πà‡∏° annotation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö recognition
                        rec_annotations.append(f"{crop_filename}\t{text}")
                        
            except Exception as e:
                print(f"Error processing line {line_num}: {e}")
                continue
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå annotation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö recognition
    rec_annotation_file = os.path.join(output_dir, 'rec_annotations.txt')
    with open(rec_annotation_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(rec_annotations))
    
    return rec_annotation_file

def crop_text_region(image, points):
    """
    ‡∏ï‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    """
    import cv2
    import numpy as np
    
    # ‡∏´‡∏≤ bounding rectangle
    points = np.array(points, dtype=np.int32)
    x, y, w, h = cv2.boundingRect(points)
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° padding
    padding = 5
    x = max(0, x - padding)
    y = max(0, y - padding)
    w = min(image.shape[1] - x, w + 2 * padding)
    h = min(image.shape[0] - y, h + 2 * padding)
    
    # ‡∏ï‡∏±‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    cropped = image[y:y+h, x:x+w]
    
    return cropped if cropped.size > 0 else None
```

---

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏**: ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PaddleOCR ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î ‡∏´‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

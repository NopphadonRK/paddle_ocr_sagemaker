{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa9c385",
   "metadata": {},
   "source": [
    "# üöÄ PaddleOCR Recognition Training on AWS SageMaker\n",
    "\n",
    "## üìã Overview\n",
    "This notebook is specifically designed to run on **AWS SageMaker** instances.\n",
    "\n",
    "### üéØ Key Features:\n",
    "- Optimized for SageMaker environment\n",
    "- Automatic AWS credentials detection\n",
    "- Built-in S3 integration\n",
    "- GPU-optimized training\n",
    "- Checkpoint management\n",
    "\n",
    "### üìä Requirements:\n",
    "- **Instance Type**: `ml.g4dn.xlarge` or higher\n",
    "- **Data**: Pre-uploaded to S3 bucket\n",
    "- **Kernel**: `conda_python3` or `Python 3`\n",
    "\n",
    "### üîß Setup Instructions:\n",
    "1. Upload this notebook to your SageMaker instance\n",
    "2. Set your S3 bucket name in Cell 2\n",
    "3. Run cells sequentially\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SAGEMAKER ENVIRONMENT SETUP =====\n",
    "# üîß Cell 1: SageMaker Environment & Dependencies Setup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ PaddleOCR Recognition Training on AWS SageMaker\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô SageMaker\n",
    "print(f\"üìç Python version: {sys.version}\")\n",
    "print(f\"üìç Working directory: {os.getcwd()}\")\n",
    "print(f\"üìç Available CPUs: {os.cpu_count()}\")\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö SageMaker environment\n",
    "sagemaker_indicators = [\n",
    "    '/opt/ml' in os.getcwd(),\n",
    "    'SageMaker' in os.environ.get('SM_FRAMEWORK_MODULE', ''),\n",
    "    os.path.exists('/opt/ml'),\n",
    "    'SAGEMAKER_REGION' in os.environ\n",
    "]\n",
    "\n",
    "if any(sagemaker_indicators):\n",
    "    print(\"‚úÖ Running on AWS SageMaker\")\n",
    "    IS_SAGEMAKER = True\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Not detected as SageMaker environment\")\n",
    "    print(\"   This notebook is optimized for SageMaker\")\n",
    "    IS_SAGEMAKER = False\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "print(\"\\nüì¶ Installing required packages...\")\n",
    "required_packages = [\n",
    "    \"paddlepaddle-gpu==2.5.2\",\n",
    "    \"boto3\",\n",
    "    \"sagemaker\",\n",
    "    \"opencv-python\",\n",
    "    \"Pillow\",\n",
    "    \"numpy\",\n",
    "    \"PyYAML\",\n",
    "    \"tqdm\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    print(f\"üì¶ Installing {package}...\")\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], \n",
    "            check=True, \n",
    "            capture_output=True\n",
    "        )\n",
    "        print(f\"  ‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Warning installing {package}: {e}\")\n",
    "\n",
    "# Import libraries\n",
    "print(\"\\nüìö Importing libraries...\")\n",
    "try:\n",
    "    import boto3\n",
    "    import sagemaker\n",
    "    import paddle\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import yaml\n",
    "    from tqdm import tqdm\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"‚úÖ All libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    raise\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU\n",
    "print(\"\\nüîç GPU Status Check:\")\n",
    "try:\n",
    "    if paddle.is_compiled_with_cuda():\n",
    "        gpu_count = paddle.device.cuda.device_count()\n",
    "        print(f\"‚úÖ PaddlePaddle GPU support available\")\n",
    "        print(f\"‚úÖ Available GPUs: {gpu_count}\")\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î GPU\n",
    "        for i in range(gpu_count):\n",
    "            paddle.device.set_device(f'gpu:{i}')\n",
    "            place = paddle.CUDAPlace(i)\n",
    "            print(f\"   GPU {i}: Ready\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  PaddlePaddle CPU version detected\")\n",
    "        print(\"   Training will be slower without GPU\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GPU check error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment setup completed!\")\n",
    "print(\"üìù Next: Configure your S3 bucket in Cell 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== AWS & S3 CONFIGURATION =====\n",
    "# üîß Cell 2: AWS Credentials & S3 Setup\n",
    "\n",
    "print(\"üîê AWS Configuration Setup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# üö® CONFIGURE YOUR S3 BUCKET HERE\n",
    "S3_BUCKET = \"sagemaker-ocr-train-bucket\"  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô bucket ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "S3_DATA_PREFIX = \"recognition-data\"\n",
    "AWS_REGION = \"ap-southeast-1\"  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô region ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "\n",
    "print(f\"üìÅ S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"üìÇ Data Prefix: {S3_DATA_PREFIX}\")\n",
    "print(f\"üåç AWS Region: {AWS_REGION}\")\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ environment variables\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "os.environ['S3_BUCKET'] = S3_BUCKET\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö AWS credentials\n",
    "print(\"\\nüîç Checking AWS credentials...\")\n",
    "try:\n",
    "    # SageMaker ‡∏à‡∏∞‡∏°‡∏µ built-in credentials\n",
    "    sts = boto3.client('sts', region_name=AWS_REGION)\n",
    "    identity = sts.get_caller_identity()\n",
    "    \n",
    "    print(f\"‚úÖ AWS credentials valid\")\n",
    "    print(f\"   Account ID: {identity['Account']}\")\n",
    "    print(f\"   User/Role: {identity['Arn'].split('/')[-1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå AWS credentials error: {e}\")\n",
    "    print(\"üí° Make sure your SageMaker instance has proper IAM role\")\n",
    "    raise\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö S3 access\n",
    "print(\"\\nüîç Checking S3 access...\")\n",
    "try:\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "    s3_resource = boto3.resource('s3', region_name=AWS_REGION)\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö bucket\n",
    "    s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "    print(f\"‚úÖ S3 bucket accessible: {S3_BUCKET}\")\n",
    "    \n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô bucket\n",
    "    bucket = s3_resource.Bucket(S3_BUCKET)\n",
    "    objects = list(bucket.objects.filter(Prefix=S3_DATA_PREFIX).limit(5))\n",
    "    \n",
    "    if objects:\n",
    "        print(f\"‚úÖ Found {len(objects)} objects in {S3_DATA_PREFIX}/\")\n",
    "        print(\"   Sample objects:\")\n",
    "        for obj in objects[:3]:\n",
    "            print(f\"   - {obj.key}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No objects found in {S3_DATA_PREFIX}/\")\n",
    "        print(\"   Please make sure data is uploaded to S3\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå S3 access error: {e}\")\n",
    "    print(\"üí° Check bucket name and region settings\")\n",
    "    raise\n",
    "\n",
    "# Initialize SageMaker session\n",
    "print(\"\\nüöÄ Initializing SageMaker session...\")\n",
    "try:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = sagemaker.get_execution_role()\n",
    "    \n",
    "    print(f\"‚úÖ SageMaker session initialized\")\n",
    "    print(f\"   Default bucket: {sagemaker_session.default_bucket()}\")\n",
    "    print(f\"   Execution role: {role.split('/')[-1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  SageMaker session warning: {e}\")\n",
    "    print(\"   This is normal if not running in SageMaker training job\")\n",
    "\n",
    "print(\"\\n‚úÖ AWS & S3 configuration completed!\")\n",
    "print(\"üìù Next: Download PaddleOCR repository in Cell 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675dfaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PADDLEOCR REPOSITORY SETUP =====\n",
    "# üîß Cell 3: Clone & Setup PaddleOCR Repository\n",
    "\n",
    "print(\"üì• PaddleOCR Repository Setup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "PADDLEOCR_DIR = \"/tmp/PaddleOCR\"\n",
    "WORK_DIR = \"/tmp/ocr_training\"\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á working directory\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Clone PaddleOCR repository\n",
    "if os.path.exists(PADDLEOCR_DIR):\n",
    "    print(f\"üìÇ PaddleOCR already exists at {PADDLEOCR_DIR}\")\n",
    "    print(\"üîÑ Updating repository...\")\n",
    "    os.chdir(PADDLEOCR_DIR)\n",
    "    subprocess.run([\"git\", \"pull\"], capture_output=True)\n",
    "else:\n",
    "    print(\"üì• Cloning PaddleOCR repository...\")\n",
    "    result = subprocess.run([\n",
    "        \"git\", \"clone\", \n",
    "        \"https://github.com/PaddlePaddle/PaddleOCR.git\",\n",
    "        PADDLEOCR_DIR\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ PaddleOCR repository cloned successfully\")\n",
    "    else:\n",
    "        print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "        raise Exception(\"Failed to clone PaddleOCR\")\n",
    "\n",
    "# ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô PaddleOCR directory\n",
    "os.chdir(PADDLEOCR_DIR)\n",
    "print(f\"üìç Current directory: {os.getcwd()}\")\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PaddleOCR requirements\n",
    "print(\"\\nüì¶ Installing PaddleOCR requirements...\")\n",
    "try:\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"requirements.txt\"\n",
    "    ], check=True, capture_output=True)\n",
    "    print(\"‚úÖ PaddleOCR requirements installed\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ö†Ô∏è  Some requirements may have failed to install: {e}\")\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö available configs\n",
    "print(\"\\nüîç Available Recognition configs:\")\n",
    "rec_configs = Path(\"configs/rec\").glob(\"**/*.yml\")\n",
    "available_configs = []\n",
    "\n",
    "for config in rec_configs:\n",
    "    if any(arch in config.name.lower() for arch in ['crnn', 'svtr', 'pp-ocr']):\n",
    "        available_configs.append(str(config))\n",
    "        print(f\"  üìÑ {config}\")\n",
    "\n",
    "if available_configs:\n",
    "    print(f\"\\n‚úÖ Found {len(available_configs)} recognition configs\")\n",
    "    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å default config\n",
    "    DEFAULT_CONFIG = \"configs/rec/PP-OCRv4/en_PP-OCRv4_rec.yml\"\n",
    "    if os.path.exists(DEFAULT_CONFIG):\n",
    "        print(f\"üéØ Default config: {DEFAULT_CONFIG}\")\n",
    "    else:\n",
    "        DEFAULT_CONFIG = available_configs[0]\n",
    "        print(f\"üéØ Using first available: {DEFAULT_CONFIG}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No recognition configs found\")\n",
    "    DEFAULT_CONFIG = None\n",
    "\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏° PaddleOCR ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Python path\n",
    "if PADDLEOCR_DIR not in sys.path:\n",
    "    sys.path.insert(0, PADDLEOCR_DIR)\n",
    "    print(f\"‚úÖ Added {PADDLEOCR_DIR} to Python path\")\n",
    "\n",
    "print(\"\\n‚úÖ PaddleOCR repository setup completed!\")\n",
    "print(\"üìù Next: Download training data from S3 in Cell 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA DOWNLOAD FROM S3 =====\n",
    "# üîß Cell 4: Download Training Data from S3\n",
    "\n",
    "print(\"üì• Downloading Training Data from S3\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "LOCAL_DATA_DIR = f\"{WORK_DIR}/data\"\n",
    "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Local data directory: {LOCAL_DATA_DIR}\")\n",
    "print(f\"‚òÅÔ∏è  S3 source: s3://{S3_BUCKET}/{S3_DATA_PREFIX}/\")\n",
    "\n",
    "def download_s3_folder(bucket, prefix, local_dir):\n",
    "    \"\"\"Download entire S3 folder with progress bar\"\"\"\n",
    "    \n",
    "    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    print(f\"üîç Scanning files in s3://{bucket}/{prefix}/...\")\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    total_files = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        if 'Contents' in page:\n",
    "            total_files += len(page['Contents'])\n",
    "            total_size += sum(obj['Size'] for obj in page['Contents'])\n",
    "    \n",
    "    print(f\"üìä Found {total_files} files ({total_size / 1024 / 1024:.1f} MB)\")\n",
    "    \n",
    "    if total_files == 0:\n",
    "        print(f\"‚ö†Ô∏è  No files found in s3://{bucket}/{prefix}/\")\n",
    "        return\n",
    "    \n",
    "    # Download files with progress bar\n",
    "    downloaded = 0\n",
    "    progress_bar = tqdm(total=total_files, desc=\"Downloading\", unit=\"files\")\n",
    "    \n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                key = obj['Key']\n",
    "                local_file = os.path.join(local_dir, key.replace(prefix + '/', ''))\n",
    "                \n",
    "                # ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ\n",
    "                os.makedirs(os.path.dirname(local_file), exist_ok=True)\n",
    "                \n",
    "                # Download file\n",
    "                try:\n",
    "                    s3_client.download_file(bucket, key, local_file)\n",
    "                    downloaded += 1\n",
    "                    progress_bar.update(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Failed to download {key}: {e}\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(f\"‚úÖ Downloaded {downloaded}/{total_files} files\")\n",
    "    return downloaded\n",
    "\n",
    "# Download ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "try:\n",
    "    downloaded_count = download_s3_folder(S3_BUCKET, S3_DATA_PREFIX, LOCAL_DATA_DIR)\n",
    "    \n",
    "    if downloaded_count > 0:\n",
    "        print(f\"\\n‚úÖ Data download completed!\")\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "        important_files = [\n",
    "            \"annotations/train_annotation.txt\",\n",
    "            \"annotations/val_annotation.txt\", \n",
    "            \"metadata/character_dict.txt\",\n",
    "            \"metadata/dataset_info.json\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüîç Checking important files:\")\n",
    "        for file_path in important_files:\n",
    "            full_path = os.path.join(LOCAL_DATA_DIR, file_path)\n",
    "            if os.path.exists(full_path):\n",
    "                size = os.path.getsize(full_path)\n",
    "                print(f\"  ‚úÖ {file_path} ({size} bytes)\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå {file_path} - missing\")\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• dataset\n",
    "        metadata_file = os.path.join(LOCAL_DATA_DIR, \"metadata/dataset_info.json\")\n",
    "        if os.path.exists(metadata_file):\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                dataset_info = json.load(f)\n",
    "            \n",
    "            print(f\"\\nüìä Dataset Information:\")\n",
    "            print(f\"   Total images: {dataset_info.get('total_images', 'Unknown')}\")\n",
    "            print(f\"   Training: {dataset_info.get('train_count', 'Unknown')}\")\n",
    "            print(f\"   Validation: {dataset_info.get('val_count', 'Unknown')}\")\n",
    "            print(f\"   Characters: {dataset_info.get('character_count', 'Unknown')}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No data downloaded. Please check S3 bucket and permissions.\")\n",
    "        raise Exception(\"Data download failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data download error: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nüìù Next: Create training configuration in Cell 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINING CONFIGURATION =====\n",
    "# üîß Cell 5: Create Training Configuration\n",
    "\n",
    "print(\"‚öôÔ∏è  Creating Training Configuration\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "CONFIG_DIR = f\"{WORK_DIR}/configs\"\n",
    "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î base config\n",
    "if DEFAULT_CONFIG and os.path.exists(DEFAULT_CONFIG):\n",
    "    print(f\"üìÑ Loading base config: {DEFAULT_CONFIG}\")\n",
    "    with open(DEFAULT_CONFIG, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using minimal config template\")\n",
    "    config = {\n",
    "        'Global': {},\n",
    "        'Architecture': {},\n",
    "        'Loss': {},\n",
    "        'Optimizer': {},\n",
    "        'Train': {'dataset': {}},\n",
    "        'Eval': {'dataset': {}}\n",
    "    }\n",
    "\n",
    "# Update config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SageMaker\n",
    "print(\"üîß Updating configuration for SageMaker environment...\")\n",
    "\n",
    "# Global settings\n",
    "config['Global'].update({\n",
    "    'debug': False,\n",
    "    'use_gpu': True,\n",
    "    'epoch_num': 10,  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epochs\n",
    "    'log_smooth_window': 20,\n",
    "    'print_batch_step': 10,\n",
    "    'save_model_dir': f'{WORK_DIR}/output/models',\n",
    "    'save_epoch_step': 2,\n",
    "    'eval_batch_step': [0, 500],\n",
    "    'cal_metric_during_train': True,\n",
    "    'pretrained_model': None,\n",
    "    'checkpoints': None,\n",
    "    'save_inference_dir': f'{WORK_DIR}/output/inference',\n",
    "    'use_visualdl': False,\n",
    "    'infer_img': None,\n",
    "    'character_dict_path': f'{LOCAL_DATA_DIR}/metadata/character_dict.txt',\n",
    "    'max_text_length': 25,\n",
    "    'infer_mode': False,\n",
    "    'use_space_char': True,\n",
    "    'distributed': False\n",
    "})\n",
    "\n",
    "# Architecture - CRNN with ResNet backbone\n",
    "config['Architecture'] = {\n",
    "    'model_type': 'rec',\n",
    "    'algorithm': 'CRNN',\n",
    "    'Transform': None,\n",
    "    'Backbone': {\n",
    "        'name': 'MobileNetV3',\n",
    "        'scale': 0.5,\n",
    "        'model_name': 'small',\n",
    "        'small_stride': [1, 2, 2, 2]\n",
    "    },\n",
    "    'Neck': {\n",
    "        'name': 'SequenceEncoder',\n",
    "        'encoder_type': 'rnn',\n",
    "        'hidden_size': 48\n",
    "    },\n",
    "    'Head': {\n",
    "        'name': 'CTCHead',\n",
    "        'fc_decay': 0.00001\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loss function\n",
    "config['Loss'] = {\n",
    "    'name': 'CTCLoss'\n",
    "}\n",
    "\n",
    "# Optimizer\n",
    "config['Optimizer'] = {\n",
    "    'name': 'Adam',\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'lr': {\n",
    "        'name': 'Cosine',\n",
    "        'learning_rate': 0.001,\n",
    "        'warmup_epoch': 2\n",
    "    },\n",
    "    'regularizer': {\n",
    "        'name': 'L2',\n",
    "        'factor': 1e-06\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training dataset\n",
    "config['Train'] = {\n",
    "    'dataset': {\n",
    "        'name': 'SimpleDataSet',\n",
    "        'data_dir': f'{LOCAL_DATA_DIR}/images/train',\n",
    "        'label_file_list': [f'{LOCAL_DATA_DIR}/annotations/train_annotation.txt'],\n",
    "        'ratio_list': [1.0]\n",
    "    },\n",
    "    'loader': {\n",
    "        'shuffle': True,\n",
    "        'batch_size_per_card': 32,\n",
    "        'drop_last': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    'transforms': [\n",
    "        {'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}},\n",
    "        {'RecAug': {}},\n",
    "        {'CTCLabelEncode': {}},\n",
    "        {'RecResizeImg': {'image_shape': [3, 32, 320]}},\n",
    "        {'KeepKeys': {'keep_keys': ['image', 'label', 'length']}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Evaluation dataset\n",
    "config['Eval'] = {\n",
    "    'dataset': {\n",
    "        'name': 'SimpleDataSet',\n",
    "        'data_dir': f'{LOCAL_DATA_DIR}/images/val',\n",
    "        'label_file_list': [f'{LOCAL_DATA_DIR}/annotations/val_annotation.txt']\n",
    "    },\n",
    "    'loader': {\n",
    "        'shuffle': False,\n",
    "        'drop_last': False,\n",
    "        'batch_size_per_card': 32,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    'transforms': [\n",
    "        {'DecodeImage': {'img_mode': 'BGR', 'channel_first': False}},\n",
    "        {'CTCLabelEncode': {}},\n",
    "        {'RecResizeImg': {'image_shape': [3, 32, 320]}},\n",
    "        {'KeepKeys': {'keep_keys': ['image', 'label', 'length']}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å config\n",
    "config_file = f\"{CONFIG_DIR}/sagemaker_rec_config.yml\"\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"‚úÖ Configuration saved: {config_file}\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á output directories\n",
    "output_dirs = [\n",
    "    config['Global']['save_model_dir'],\n",
    "    config['Global']['save_inference_dir'],\n",
    "    f\"{WORK_DIR}/logs\"\n",
    "]\n",
    "\n",
    "for dir_path in output_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"üìÅ Created: {dir_path}\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ config\n",
    "print(f\"\\nüìã Training Configuration Summary:\")\n",
    "print(f\"   Algorithm: {config['Architecture']['algorithm']}\")\n",
    "print(f\"   Backbone: {config['Architecture']['Backbone']['name']}\")\n",
    "print(f\"   Epochs: {config['Global']['epoch_num']}\")\n",
    "print(f\"   Batch size: {config['Train']['loader']['batch_size_per_card']}\")\n",
    "print(f\"   Learning rate: {config['Optimizer']['lr']['learning_rate']}\")\n",
    "print(f\"   Character dict: {config['Global']['character_dict_path']}\")\n",
    "print(f\"   Model output: {config['Global']['save_model_dir']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training configuration completed!\")\n",
    "print(\"üìù Next: Start training in Cell 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdccfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== START TRAINING =====\n",
    "# üîß Cell 6: Start PaddleOCR Recognition Training\n",
    "\n",
    "print(\"üöÄ Starting PaddleOCR Recognition Training\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô\n",
    "print(\"üîç Pre-training checks:\")\n",
    "\n",
    "required_files = [\n",
    "    config_file,\n",
    "    config['Global']['character_dict_path'],\n",
    "    config['Train']['dataset']['label_file_list'][0],\n",
    "    config['Eval']['dataset']['label_file_list'][0]\n",
    "]\n",
    "\n",
    "all_files_exist = True\n",
    "for file_path in required_files:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"  ‚úÖ {os.path.basename(file_path)}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {file_path} - missing\")\n",
    "        all_files_exist = False\n",
    "\n",
    "if not all_files_exist:\n",
    "    raise Exception(\"Required files missing. Please run previous cells.\")\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "if paddle.is_compiled_with_cuda():\n",
    "    gpu_count = paddle.device.cuda.device_count()\n",
    "    print(f\"  ‚úÖ GPU available: {gpu_count} device(s)\")\n",
    "    paddle.device.set_device('gpu:0')\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Training on CPU (will be slower)\")\n",
    "    paddle.device.set_device('cpu')\n",
    "\n",
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á PaddleOCR directory\n",
    "os.chdir(PADDLEOCR_DIR)\n",
    "print(f\"üìç Training from: {os.getcwd()}\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á training command\n",
    "training_cmd = [\n",
    "    sys.executable, \n",
    "    \"tools/train.py\",\n",
    "    \"-c\", config_file,\n",
    "    \"-o\", \"Global.use_gpu=True\",\n",
    "    \"-o\", \"Global.epoch_num=10\",\n",
    "    \"-o\", \"Train.loader.batch_size_per_card=16\",  # ‡∏•‡∏î batch size ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SageMaker\n",
    "    \"-o\", \"Global.print_batch_step=5\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ Training Command:\")\n",
    "print(f\"   {' '.join(training_cmd)}\")\n",
    "\n",
    "print(f\"\\nüèÅ Starting training... (This will take some time)\")\n",
    "print(f\"üìä Monitor GPU usage with: watch -n 1 nvidia-smi\")\n",
    "print(f\"üìà Training progress will be displayed below:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"‚è∞ Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    # ‡∏£‡∏±‡∏ô training ‡πÅ‡∏ö‡∏ö real-time output\n",
    "    process = subprocess.Popen(\n",
    "        training_cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• output ‡πÅ‡∏ö‡∏ö real-time\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        if line:\n",
    "            print(line.rstrip())\n",
    "    \n",
    "    # ‡∏£‡∏≠‡πÉ‡∏´‡πâ process ‡πÄ‡∏™‡∏£‡πá‡∏à\n",
    "    process.wait()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"üéâ Training completed successfully!\")\n",
    "        print(f\"‚è±Ô∏è  Total training time: {duration}\")\n",
    "        print(f\"üìÅ Models saved in: {config['Global']['save_model_dir']}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Training failed with return code: {process.returncode}\")\n",
    "        raise Exception(\"Training process failed\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "    process.terminate()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nüìù Next: Test trained model in Cell 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e6b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL TESTING & VALIDATION =====\n",
    "# üîß Cell 7: Test Trained Model\n",
    "\n",
    "print(\"üß™ Testing Trained Model\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "MODEL_DIR = config['Global']['save_model_dir']\n",
    "print(f\"üìÅ Model directory: {MODEL_DIR}\")\n",
    "\n",
    "# ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    model_files = [f for f in os.listdir(MODEL_DIR) if f.startswith('latest') or f.startswith('best')]\n",
    "    if model_files:\n",
    "        latest_model = sorted(model_files)[-1]\n",
    "        model_path = os.path.join(MODEL_DIR, latest_model)\n",
    "        print(f\"üéØ Using model: {latest_model}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No trained models found\")\n",
    "        model_path = None\n",
    "else:\n",
    "    print(\"‚ùå Model directory not found\")\n",
    "    model_path = None\n",
    "\n",
    "if model_path:\n",
    "    print(\"\\nüîç Running model evaluation...\")\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á evaluation command\n",
    "    eval_cmd = [\n",
    "        sys.executable,\n",
    "        \"tools/eval.py\",\n",
    "        \"-c\", config_file,\n",
    "        \"-o\", f\"Global.checkpoints={model_path}\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìä Evaluation command: {' '.join(eval_cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(eval_cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Evaluation completed\")\n",
    "            print(\"üìà Results:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Evaluation failed: {result.stderr}\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ö†Ô∏è  Evaluation timeout (5 minutes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation error: {e}\")\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ inference ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢\n",
    "print(\"\\nüñºÔ∏è  Testing inference on sample images...\")\n",
    "\n",
    "sample_images_dir = f\"{LOCAL_DATA_DIR}/images/val\"\n",
    "if os.path.exists(sample_images_dir):\n",
    "    sample_images = [f for f in os.listdir(sample_images_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))][:3]\n",
    "    \n",
    "    for img_name in sample_images:\n",
    "        img_path = os.path.join(sample_images_dir, img_name)\n",
    "        print(f\"\\nüîç Testing: {img_name}\")\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á inference command\n",
    "        infer_cmd = [\n",
    "            sys.executable,\n",
    "            \"tools/infer_rec.py\",\n",
    "            \"-c\", config_file,\n",
    "            \"-o\", f\"Global.checkpoints={model_path}\",\n",
    "            \"-o\", f\"Global.infer_img={img_path}\"\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(infer_cmd, capture_output=True, text=True, timeout=30)\n",
    "            if result.returncode == 0:\n",
    "                # ‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å output\n",
    "                output_lines = result.stdout.strip().split('\\n')\n",
    "                for line in output_lines:\n",
    "                    if 'result:' in line.lower() or 'text:' in line.lower():\n",
    "                        print(f\"  üìù {line}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Inference failed: {result.stderr}\")\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"  ‚ö†Ô∏è  Inference timeout for {img_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Inference error: {e}\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "print(\"\\nüìä Training Summary:\")\n",
    "print(f\"   Model saved: {MODEL_DIR}\")\n",
    "print(f\"   Config used: {config_file}\")\n",
    "print(f\"   Character dict: {config['Global']['character_dict_path']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model testing completed!\")\n",
    "print(\"üìù Next: Upload results to S3 in Cell 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84826c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== UPLOAD RESULTS TO S3 =====\n",
    "# üîß Cell 8: Upload Training Results to S3\n",
    "\n",
    "print(\"‚òÅÔ∏è  Uploading Training Results to S3\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "S3_OUTPUT_PREFIX = \"training-results\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_prefix = f\"{S3_OUTPUT_PREFIX}/run_{timestamp}\"\n",
    "\n",
    "print(f\"üìÅ S3 destination: s3://{S3_BUCKET}/{results_prefix}/\")\n",
    "\n",
    "def upload_directory_to_s3(local_dir, bucket, s3_prefix):\n",
    "    \"\"\"Upload directory to S3 with progress tracking\"\"\"\n",
    "    \n",
    "    if not os.path.exists(local_dir):\n",
    "        print(f\"‚ö†Ô∏è  Directory not found: {local_dir}\")\n",
    "        return 0\n",
    "    \n",
    "    # ‡∏ô‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            local_file = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_file, local_dir)\n",
    "            s3_key = f\"{s3_prefix}/{relative_path}\".replace('\\\\', '/')\n",
    "            all_files.append((local_file, s3_key))\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"‚ö†Ô∏è  No files found in {local_dir}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"üì§ Uploading {len(all_files)} files...\")\n",
    "    \n",
    "    uploaded = 0\n",
    "    progress_bar = tqdm(all_files, desc=\"Uploading\", unit=\"files\")\n",
    "    \n",
    "    for local_file, s3_key in progress_bar:\n",
    "        try:\n",
    "            s3_client.upload_file(local_file, bucket, s3_key)\n",
    "            uploaded += 1\n",
    "            progress_bar.set_postfix({\"uploaded\": uploaded})\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Failed to upload {local_file}: {e}\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(f\"‚úÖ Uploaded {uploaded}/{len(all_files)} files\")\n",
    "    return uploaded\n",
    "\n",
    "# Upload trained models\n",
    "print(\"\\nüì¶ Uploading trained models...\")\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    model_count = upload_directory_to_s3(MODEL_DIR, S3_BUCKET, f\"{results_prefix}/models\")\n",
    "    if model_count > 0:\n",
    "        print(f\"‚úÖ Models uploaded: s3://{S3_BUCKET}/{results_prefix}/models/\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No models to upload\")\n",
    "\n",
    "# Upload configuration\n",
    "print(\"\\n‚öôÔ∏è  Uploading configuration...\")\n",
    "try:\n",
    "    config_s3_key = f\"{results_prefix}/config/sagemaker_rec_config.yml\"\n",
    "    s3_client.upload_file(config_file, S3_BUCKET, config_s3_key)\n",
    "    print(f\"‚úÖ Config uploaded: s3://{S3_BUCKET}/{config_s3_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Config upload failed: {e}\")\n",
    "\n",
    "# Upload logs (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "log_dir = f\"{WORK_DIR}/logs\"\n",
    "if os.path.exists(log_dir) and os.listdir(log_dir):\n",
    "    print(\"\\nüìÑ Uploading logs...\")\n",
    "    log_count = upload_directory_to_s3(log_dir, S3_BUCKET, f\"{results_prefix}/logs\")\n",
    "    if log_count > 0:\n",
    "        print(f\"‚úÖ Logs uploaded: s3://{S3_BUCKET}/{results_prefix}/logs/\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ upload summary report\n",
    "print(\"\\nüìä Creating training summary...\")\n",
    "summary_report = {\n",
    "    \"training_info\": {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"duration\": str(datetime.now() - start_time) if 'start_time' in globals() else \"Unknown\",\n",
    "        \"algorithm\": config['Architecture']['algorithm'],\n",
    "        \"backbone\": config['Architecture']['Backbone']['name'],\n",
    "        \"epochs\": config['Global']['epoch_num'],\n",
    "        \"batch_size\": config['Train']['loader']['batch_size_per_card'],\n",
    "        \"learning_rate\": config['Optimizer']['lr']['learning_rate']\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"train_annotation\": config['Train']['dataset']['label_file_list'][0],\n",
    "        \"val_annotation\": config['Eval']['dataset']['label_file_list'][0],\n",
    "        \"character_dict\": config['Global']['character_dict_path']\n",
    "    },\n",
    "    \"s3_locations\": {\n",
    "        \"models\": f\"s3://{S3_BUCKET}/{results_prefix}/models/\",\n",
    "        \"config\": f\"s3://{S3_BUCKET}/{results_prefix}/config/\",\n",
    "        \"logs\": f\"s3://{S3_BUCKET}/{results_prefix}/logs/\"\n",
    "    },\n",
    "    \"environment\": {\n",
    "        \"platform\": \"AWS SageMaker\",\n",
    "        \"gpu_available\": paddle.is_compiled_with_cuda(),\n",
    "        \"python_version\": sys.version,\n",
    "        \"paddlepaddle_version\": paddle.__version__\n",
    "    }\n",
    "}\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞ upload summary\n",
    "summary_file = f\"{WORK_DIR}/training_summary.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    summary_s3_key = f\"{results_prefix}/training_summary.json\"\n",
    "    s3_client.upload_file(summary_file, S3_BUCKET, summary_s3_key)\n",
    "    print(f\"‚úÖ Summary uploaded: s3://{S3_BUCKET}/{summary_s3_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Summary upload failed: {e}\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Training Summary:\")\n",
    "print(f\"   Timestamp: {timestamp}\")\n",
    "print(f\"   Algorithm: {config['Architecture']['algorithm']}\")\n",
    "print(f\"   Epochs: {config['Global']['epoch_num']}\")\n",
    "print(f\"\")\n",
    "print(f\"‚òÅÔ∏è  Results uploaded to S3:\")\n",
    "print(f\"   üìÅ Models: s3://{S3_BUCKET}/{results_prefix}/models/\")\n",
    "print(f\"   ‚öôÔ∏è  Config: s3://{S3_BUCKET}/{results_prefix}/config/\")\n",
    "print(f\"   üìÑ Summary: s3://{S3_BUCKET}/{results_prefix}/training_summary.json\")\n",
    "print(f\"\")\n",
    "print(f\"üîÑ To use trained model:\")\n",
    "print(f\"   1. Download from S3\")\n",
    "print(f\"   2. Use with PaddleOCR inference tools\")\n",
    "print(f\"   3. Character dict: {config['Global']['character_dict_path']}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
